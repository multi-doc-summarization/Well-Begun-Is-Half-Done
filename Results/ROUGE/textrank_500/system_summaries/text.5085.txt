b wide crawl with initial seedlist and crawler configuration from march 2011 . newline_char this uses the new hq software for distributed crawling by kenji nagahashi . newline_char what ’ s in the data set : crawl start date : 09 march , 2011 crawl end date : 23 december , 2011 number of captures : 2,713,676,341 number of unique urls : 2,273,840,159 number of hosts : 29,032,069 the seed list for this crawl was a list of alexa ’ s top 1 million web sites , retrieved close to the crawl start date . newline_char we used heritrix ( 3.1.1-snapshot ) crawler software and respected robots.txt directives . newline_char the scope of the crawl was not limited except for a few manually excluded sites . newline_char however this was a somewhat experimental crawl for us , as we were using newly minted software to feed urls to the crawlers , and we know there were some operational issues with it . newline_char for example , in many cases we may not have crawled all of the embedded and linked objects in a page since the urls for these resources were added into queues that quickly grew bigger than the intended size of the crawl ( and therefore we never got to them ) . newline_char we also included repeated crawls of some argentinian government sites , so looking at results by country will be somewhat skewed . newline_char we have made many changes to how we do these wide crawls since this particular example , but we wanted to make the data available “ warts and all ” for people to experiment with . newline_char if you would like access to this set of crawl data , please contact us at info at archive dot org and let us know who you are and what you ’ re hoping to do with it . story_separator_special_tag 
